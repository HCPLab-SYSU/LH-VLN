<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method"/>
  <meta property="og:description" content="Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method"/>
  <meta property="og:url" content="LH-VLN.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LH-VLN</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Xinshuai Song</a><sup>1*</sup>,</span> -->
                Xinshuai Song</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Weixing Chen</a><sup>1*</sup>,</span> -->
                  Weixing Chen</a><sup>1*</sup>,</span>
                    <span class="author-block">
                      <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yang Liu</a><sup>1†</sup>,</span> -->
                      Yang Liu</a><sup>1†</sup>,</span>
                      <span class="author-block">
                        <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Weikai Chen</a><sup>2</sup>,</span> -->
                        Weikai Chen</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <!-- <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Guanbin Li</a><sup>1</sup>,</span><br> -->
                          Guanbin Li</a><sup>1,3</sup>,</span>
                          <span class="author-block">
                            <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Liang Lin</a><sup>1</sup>,</span> -->
                            Liang Lin</a><sup>1,3</sup></span>
                  <!-- </span> -->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Sun Yat-sen University</span>
                    <span class="author-block"><sup>2</sup>Tencent America</span>
                    <span class="author-block"><sup>3</sup>Pengcheng Laboratory</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><sup>†</sup>Corresponding Author</small></span>
                    <!-- <span class="eql-cntrb"><small><sup>‡</sup>Corresponding Author</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> 

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> 

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/HCPLab-SYSU/LH-VLN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Github</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/videos/demo.gif" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
    </div>
  </div>
</section>
  

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing Vision-Language Navigation (VLN) methods primarily focus on single-stage navigation, limiting their effectiveness in multi-stage and long-horizon tasks within complex and dynamic environments. To address these limitations, we propose a novel VLN task, named Long-Horizon Vision-Language Navigation (LH-VLN), which emphasizes long-term planning and decision consistency across consecutive subtasks. Furthermore, to support LH-VLN, we develop an automated data generation platform NavGen, which constructs datasets with complex task structures and improves data utility through a bidirectional, multi-granularity generation approach. To accurately evaluate complex tasks, we construct the Long-Horizon Planning and Reasoning in VLN (LHPR-VLN) benchmark consisting of 3,260 tasks with an average of 150 task steps, serving as the first dataset specifically designed for the long-horizon vision-language navigation task. Furthermore, we propose Independent Success Rate (ISR), Conditional Success Rate (CSR), and CSR weight by Ground Truth (CGT) metrics, to provide fine-grained assessments of task completion. To improve model adaptability in complex tasks, we propose a novel Multi-Granularity Dynamic Memory (MGDM) module that integrates short-term memory blurring with long-term memory retrieval to enable flexible navigation in dynamic environments. Our platform, benchmark and method supply LH-VLN with a robust data generation pipeline, comprehensive model evaluation dataset, reasonable metrics, and a novel VLN model, establishing a foundational framework for advancing LH-VLN.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section" id="Pipeline">
  <div class="container is-max-desktop content">
    <h2 class="title">Overview</h2>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        <b>Framework Overview. Different from existing vision language navigation, object loco-navigation, and demand-driven navigation benchmarks, LH-VLN divides navigation into multiple subtasks, requiring the agent to complete these tasks sequentially within the scene. Our data generation framework provides a general LH-VLN task generation pipeline, and the newly built LHPR-VLN benchmark for multi-stage navigation tasks. Our navigation model, based on the chain-of-thought (CoT) feedback and adaptive memory design, achieves efficient navigation by utilizing CoT prompts and dynamic long-term and short-term memories.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->


<section class="section" id="exp1">
  <div class="container is-max-desktop content">
    <h2 class="title">Navgen: an automated data generation platform</h2>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="video-compare-container" id="seq25_compare_videoDiv" style="display: inline;">
        <video class="video" id="seq25_compare_video" loop="" playsinline="" autoplay="" muted="" src="static/videos/seq_25_combine.mp4" onplay="resizeAndPlay(this)" style="height: 0px;">
        </video>
        <canvas height="1072" class="videoMerge" id="seq25_compare_videoMerge" width="1600"></canvas>
      </div>
    </div>
  </div>
</section> -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/navgen.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
       NavGen data generation platform. The forward generation generates LH-VLN complex tasks and corresponding subtasks by prompting GPT-4 with sampling asserts. The sampled assets are deployed on simulator to build the simulation environment. Based on the navigation model or expert decisions, corresponding trajectory data is generated. In the backward generation, the trajectory of each subtask is split into action-label pairs by trajectory splitting algorithm according to the trajectory type, these pairs are then input into GPT-4 to generate step-by-step tasks.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->


<section class="section" id="exp2">
  <div class="container is-max-desktop content">
    <h2 class="title">Model</h2>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/MGDM.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        The framework of the Multi-Granularity Dynamic Memory (MGDM) model. The CoT feedback module receives task instructions and, based on historical observation of corresponding memory, generates a chain of thought and constructs language prompts. The short-term memory module aims to minimize the entropy of the confidence vector, using pooling operations to forget and blur the memory sequence. The long-term memory module selects and matches data from the dataset to weight the decisions of the LLM, ultimately determining the action to be executed by the agent.
         <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->



<section class="section" id="novel_scene">
  <div class="container is-max-desktop content">
    <h2 class="title">Experiments</h2>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/exp1.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        Visualization of a successful long-horizon navigation of our MGDM. We highlight aligned landmarks by colored bounding boxes in images and words in the instruction using the same color. In the first navigation segment, the agent looks for a towel in the bathroom. It successfully finds both the bathroom and the towel but does not enter the bathroom or gets close enough to the towel for the task to be marked as successful. In the next phase, the agent successfully finds the box in the living room.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
