Path:
  scene: "LH-VLN/data/hm3d/"
  scene_dataset: "LH-VLN/data/hm3d/hm3d_annotated_basis.scene_dataset_config.json"
  
  episode_data: "LH-VLN/data/episode_task"
  task_data: "LH-VLN/data/task"
  step_task_data: "LH-VLN/data/step_task"
  train_batch: ['/batch_1', '/batch_2', '/batch_3', '/batch_4', '/batch_5']
  val_batch: ['/batch_6']
  test_batch: ['/batch_7', '/batch_8']
  split_by_scene: True

  save_checkpoints: "LH-VLN/save_checkpoints"

  tensorboard_path: "LH-VLN/run"
  output_dir: "LH-VLN/output"

Train:
  mode: 'test'
  tensorboard: False
  best_checkpoint: None
  load_checkpoint: False
  save_ckpt_per_epochs: 10

  batch_size: 1
  num_epochs: 20
  ignoreid: -100
  max_step: 500
  gradient_accumulation_step: 2
  num_warmup_steps: 1000

  success_dis: 1

Model:
  model_name: "LLM Model"
  pretrained_clip: "LH-VLN/data/models/clip-vit-base-patch16"
  bert_model: "LH-VLN/data/models/bert-large-uncased"
  pretrained_model_name_or_path: "path to your pretrained LLM(Llama/Qwen base series)"
  resume_from_checkpoint: None  # if you have a checkpoint and you want to load model from that state, here is the path you need to add
  from_scratch: True  # if your model is not vicuna 7b or llama 8b, set it to True
  feat_dropout: 0.4
  temperature: 1.0
  image_feat_size: 1024
  angle_feat_size: 4
  enc_full_graph: True
  num_pano_layers: 2
  max_memory: 20  # based on your vram

